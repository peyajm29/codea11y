Our participants mainly used the auto-complete feature only when they had a clear mental model of the desired code structure and sought to expedite the writing process. In contrast, they heavily relied on the conversational interface for syntax inquiries, conceptual understanding, and generation of code templates. We noticed our participants wrote brief, task-oriented prompts that focused on immediate code solutions or specific UI modifications, often disregarding broader architectural considerations. Their prompting style was iterative and reactive, frequently requesting small incremental changes or fixes to previous outputs. Further, their prompts lacked accessibility awareness and instead centered around visual and functional attributes (e.g., ``add a grey background to the subscription-form''(P4) or ``add a grey patch''(P1)). Consequently, the AI assistant's suggestions often failed to automatically incorporate accessibility best practices. Occasionally, our participants prompted for enhancements that indirectly aligned with accessibility requirements (e.g., ``I want to label the images or add relevant image descriptors'' (P2)). ⁤⁤In these situations, Copilot provided relevant accessibility suggestions. ⁤However, participants' over-reliance on AI assistance often led them to assume that Copilot's code output was correct and complete. ⁤⁤For instance, despite additional explanations from Copilot advising manual adjustments to image descriptions as highlighted in Table~\ref{tab:responses
}, participants frequently overlooked these suggestions and directly pasted the code, resulting in code submissions with empty \fbox{\texttt{alt
    }
} attributes. ⁤